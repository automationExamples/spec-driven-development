# Code Generation Log

This document tracks all AI-assisted code generation used in this project.

## Tools Used

- **Claude Code** (Anthropic Claude Opus 4.5) - Primary code generation tool

## Generation Sessions

### Session 1: Project Structure and Core Implementation

**Date:** 2026-02-06

**Tool:** Claude Code (Claude Opus 4.5)

**What was generated:**

1. **Feature Specifications** (`SPECS/*.md`)
   - All 6 feature specs created from requirements
   - Includes acceptance criteria for each feature

2. **Example API** (`example_api/main.py`)
   - FastAPI application with CRUD endpoints
   - In-memory storage for items

3. **OpenAPI Parser** (`app/openapi_parser.py`)
   - YAML/JSON parsing
   - Schema normalization
   - Reference resolution

4. **Test Generator** (`app/generator/`)
   - LLMClient interface and MockLLMClient
   - Test plan generation
   - Pytest file compilation

5. **Test Runner** (`app/runner/`)
   - Pytest execution wrapper
   - JUnit XML parser
   - Result aggregation

6. **Storage Layer** (`app/storage/db.py`)
   - SQLite database schema
   - Repository functions for specs, generations, runs

7. **FastAPI Routes** (`app/routes.py`)
   - REST API endpoints
   - Request/response models

8. **Streamlit UI** (`streamlit_app/`)
   - Multi-page application
   - Upload, generate, run pages

9. **Tests** (`tests/`)
   - Unit tests for parser, generator, junit_parser
   - Integration tests for API endpoints
   - E2E workflow tests

10. **Build Configuration**
    - Makefile
    - Dockerfiles (API, Streamlit, Example)
    - docker-compose.yml
    - GitHub Actions CI

**Validation/Refinement:**

- All generated code was reviewed for:
  - Syntax validity (Python AST parsing)
  - Consistent coding style
  - Proper error handling
  - Documentation coverage

- Test coverage verified through:
  - Unit tests for core components
  - Integration tests for API
  - E2E tests for full workflow

## Prompts Used

### Initial Planning
```
Create a FastAPI service that:
1. Ingests OpenAPI specs
2. Generates pytest tests (LLM-assisted)
3. Runs tests against a target API
4. Returns results with pass/fail, failures, logs

Key requirements:
- Mock mode (deterministic) for CI
- Real LLM mode (optional)
- SQLite storage
- Streamlit UI
```

### Implementation
Each component was implemented following the spec-first workflow:
1. Write feature spec in SPECS/
2. Implement according to spec
3. Add unit tests
4. Verify acceptance criteria

## Key Decisions Made by AI

1. **Mock-first approach**: MockLLMClient generates deterministic tests based on endpoint structure, not random generation

2. **Template-based generation**: Tests are generated from structured test plans, not raw LLM output, ensuring consistency

3. **Separation of concerns**: Clear boundaries between parser, generator, runner, and storage

4. **Sync execution**: Tests run synchronously to simplify the architecture (no background workers)

## Files NOT Generated by AI

- `RULES.md` - Project rules (pre-existing)
- `TODO.md` - Task tracking (pre-existing)
- `openapi_specs/example_openapi.yaml` - Hand-crafted to match example API
